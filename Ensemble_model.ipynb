{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNO9M0JvnSh/wZIf5oShUE5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NetaYinon26/projects/blob/main/Ensemble_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **predictions for FLAML, TPOT, H2O models**"
      ],
      "metadata": {
        "id": "RrxrImOq7vrl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HiK9dAkc7UPy"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "import shap\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "\n",
        "# FLAML AutoML\n",
        "from flaml import AutoML\n",
        "\n",
        "# TPOT AutoML\n",
        "from tpot import TPOTClassifier\n",
        "\n",
        "# H2O AutoML\n",
        "import h2o\n",
        "from h2o.automl import H2OAutoML\n",
        "\n",
        "# ==============\n",
        "# loading data\n",
        "# ==============\n",
        "train_df = pd.read_csv('train.csv')\n",
        "test_df = pd.read_csv('test.csv')\n",
        "\n",
        "TARGET = 'smoking'\n",
        "ID_COL = 'id'\n",
        "\n",
        "# data pre-processing\n",
        "X = train_df.drop([TARGET, ID_COL], axis=1)\n",
        "y = train_df[TARGET]\n",
        "X_test = test_df.drop(ID_COL, axis=1)\n",
        "\n",
        "# ==============\n",
        "# FLAML AutoML\n",
        "# ==============\n",
        "flaml_automl = AutoML()\n",
        "flaml_settings = {\n",
        "    \"time_budget\": 3600,\n",
        "    \"metric\": \"roc_auc\",\n",
        "    \"task\": \"classification\",\n",
        "    \"log_file_name\": \"flaml_log.log\",\n",
        "    \"seed\": 42\n",
        "}\n",
        "\n",
        "flaml_automl.fit(X, y, **flaml_settings)\n",
        "\n",
        "print(f\"FLAML Best Hyperparameters: {flaml_automl.best_config}\")\n",
        "print(f\"FLAML Best Model: {flaml_automl.best_estimator}\")\n",
        "\n",
        "# prediction for train and test set\n",
        "flaml_train_preds = flaml_automl.predict_proba(X)[:, 1]\n",
        "flaml_test_preds = flaml_automl.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# AUC for training set\n",
        "flaml_auc = roc_auc_score(y, flaml_train_preds)\n",
        "print(f\"FLAML AUC (train): {flaml_auc:.4f}\")\n",
        "\n",
        "# ==============\n",
        "# TPOT AutoML\n",
        "# ==============\n",
        "tpot = TPOTClassifier(\n",
        "    generations=3,\n",
        "    population_size=30,\n",
        "    verbosity=2,\n",
        "    scoring='roc_auc',\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "tpot.fit(X, y)\n",
        "print(f\"TPOT Best Pipeline: {tpot.fitted_pipeline_}\")\n",
        "\n",
        "# prediction for train and test set\n",
        "tpot_train_preds = tpot.predict_proba(X)[:, 1]\n",
        "tpot_test_preds  = tpot.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# AUC for training set\n",
        "tpot_auc = roc_auc_score(y, tpot_train_preds)\n",
        "print(f\"TPOT AUC (train): {tpot_auc:.4f}\")\n",
        "\n",
        "\n",
        "# ==============\n",
        "# H2O AutoML\n",
        "# ==============\n",
        "h2o.init()\n",
        "train_h2o = h2o.H2OFrame(train_df)\n",
        "train_h2o[TARGET] = train_h2o[TARGET].asfactor()\n",
        "test_h2o = h2o.H2OFrame(test_df)\n",
        "\n",
        "aml = H2OAutoML(max_runtime_secs=3600, seed=42)\n",
        "aml.train(x=X.columns.tolist(), y=TARGET, training_frame=train_h2o)\n",
        "\n",
        "# prediction for train and test set\n",
        "h2o_train_preds = aml.leader.predict(train_h2o).as_data_frame()['p1'].values\n",
        "h2o_test_preds  = aml.leader.predict(test_h2o ).as_data_frame()['p1'].values\n",
        "\n",
        "# AUC for training set\n",
        "h2o_auc = roc_auc_score(y, h2o_train_preds)\n",
        "print(f\"H2O AUC (train): {h2o_auc:.4f}\")\n",
        "\n",
        "print(\"H2O Best Model:\", aml.leader.algo)\n",
        "print(\"Hyperparameters of the best model:\")\n",
        "print(aml.leader.params)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **creating weighted voting ensemble model for predictions from 3 models**"
      ],
      "metadata": {
        "id": "rDw6d0gX7si6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# computing test predictions according to train AUC score of each model\n",
        "combined_proba = (\n",
        "    (flaml_test_preds * flaml_auc) +\n",
        "    (tpot_test_preds  * tpot_auc) +\n",
        "    (h2o_test_preds   * h2o_auc)\n",
        ") / (flaml_auc + tpot_auc + h2o_auc)\n",
        "\n",
        "# prediction for ensemble training set\n",
        "combined_train_proba = (\n",
        "                         (flaml_train_preds * flaml_auc) +\n",
        "                         (tpot_train_preds * tpot_auc) +\n",
        "                         (h2o_train_preds * h2o_auc)\n",
        "                 ) / (flaml_auc + tpot_auc + h2o_auc)\n",
        "\n",
        "combined_train_proba_auc = roc_auc_score(y, combined_train_proba)\n",
        "print(\"combined train auc: \", combined_train_proba_auc)\n",
        "\n",
        "submission_combined_proba = pd.DataFrame({\n",
        "    ID_COL: test_df[ID_COL],\n",
        "    TARGET: combined_proba\n",
        "})\n",
        "\n",
        "# ==============\n",
        "# submission files\n",
        "# ==============\n",
        "\n",
        "submission_flaml = pd.DataFrame({ID_COL: test_df[ID_COL], TARGET: flaml_test_preds})\n",
        "submission_flaml.to_csv('submission_flaml.csv', index=False)\n",
        "\n",
        "submission_tpot = pd.DataFrame({ID_COL: test_df[ID_COL], TARGET: tpot_test_preds})\n",
        "submission_tpot.to_csv('submission_tpot.csv', index=False)\n",
        "\n",
        "submission_h2o = pd.DataFrame({ID_COL: test_df[ID_COL], TARGET: h2o_test_preds})\n",
        "submission_h2o.to_csv('submission_h2o.csv', index=False)\n",
        "\n",
        "submission_combined_proba.to_csv('submission_combined_proba.csv', index=False)\n",
        "\n",
        "print(\"Saved submission_combined_proba.csv successfully!\")\n",
        "\n",
        "print(\"Saved all submission files successfully!\")\n"
      ],
      "metadata": {
        "id": "ryCd39Ic8aQg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **SHAP Analysis with KernelExplainer**"
      ],
      "metadata": {
        "id": "9eQu_nRp8hae"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) Define a function for model prediction (since KernelExplainer needs a callable function)\n",
        "def model_predict(data):\n",
        "    \"\"\"Function that takes input features and returns FLAML model predictions\"\"\"\n",
        "    data_df = pd.DataFrame(data, columns=X.columns)  # Ensure correct feature names\n",
        "    return flaml_automl.predict_proba(data_df)[:, 1]  # Return probabilities for class 1\n",
        "\n",
        "# 2) Create background data (sampling 100 random rows for efficiency)\n",
        "X_background = X.sample(100, random_state=42)\n",
        "\n",
        "# 3) Initialize KernelExplainer\n",
        "explainer = shap.KernelExplainer(\n",
        "    model=model_predict,\n",
        "    data=X_background\n",
        ")\n",
        "\n",
        "# 4) Compute SHAP values for a sample of 200 instances (to reduce computation time)\n",
        "X_shap_eval = X.sample(200, random_state=42)\n",
        "shap_values = explainer.shap_values(X_shap_eval, nsamples=100)\n",
        "\n",
        "# (Optional) Initialize JS-based visuals (if using Jupyter Notebook)\n",
        "shap.initjs()\n",
        "\n",
        "# 5) Local Explanations for 3 specific samples\n",
        "sample_indices = [0, 1, 2]  # Pick any 3 rows from the sampled data\n",
        "for idx in sample_indices:\n",
        "    row = X_shap_eval.iloc[idx]\n",
        "    # Waterfall plot for each sample\n",
        "    shap.plots.waterfall(\n",
        "        shap.Explanation(\n",
        "            values=shap_values[idx],\n",
        "            base_values=explainer.expected_value,\n",
        "            data=row,\n",
        "            feature_names=X.columns\n",
        "        )\n",
        "    )\n",
        "\n",
        "# 6) Global Feature Importance: Summary Plot\n",
        "shap.summary_plot(shap_values, X_shap_eval, feature_names=X.columns)\n",
        "\n",
        "# 7) Identify Top 5 Features by Mean Absolute SHAP Value\n",
        "mean_abs_shap = np.mean(np.abs(shap_values), axis=0)\n",
        "top5_indices  = np.argsort(mean_abs_shap)[-5:][::-1]\n",
        "top5_features = X.columns[top5_indices]\n",
        "print(\"Top 5 Important Features:\", top5_features.tolist())\n",
        "\n",
        "# 8) SHAP Dependence Plots for Top 5 Features with customized y-axis label\n",
        "for feat in top5_features:\n",
        "    # Generate the dependence plot but do not immediately display it\n",
        "    shap.dependence_plot(\n",
        "        feat,\n",
        "        shap_values,\n",
        "        X_shap_eval,\n",
        "        interaction_index=None,  # Let SHAP auto-select an interaction feature\n",
        "        show=False\n",
        "    )\n",
        "    # Customize the y-axis label\n",
        "    plt.ylabel(\"SHAP Value\")\n",
        "    plt.title(f\"SHAP Dependence Plot for {feat}\")\n",
        "    # Show the plot\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "S6Lk8dQq8myJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}